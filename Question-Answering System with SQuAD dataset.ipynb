{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question-Answering System with SQuAD dataset\n",
    "\n",
    "In this project, we will develop a question-answering system. Please note that **usage of the `Standard_NC6` GPU compute on Azure ML Studio is required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "gather": {
     "logged": 1638505430864
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-04 19:56:23.301588: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory\n",
      "2021-12-04 19:56:23.301632: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "# load necessary modules\n",
    "import re, os, json, pickle, ast, time, random, requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import scipy\n",
    "import scipy.sparse as sp\n",
    "from scipy.sparse.linalg import norm\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer \n",
    "from nltk.corpus import wordnet\n",
    "nltk.download(\"stopwords\", quiet = True)\n",
    "nltk.download(\"wordnet\", quiet = True)\n",
    "nltk.download(\"averaged_perceptron_tagger\", quiet = True)\n",
    "nltk.download(\"punkt\", quiet = True)\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import BertTokenizerFast, BertForQuestionAnswering\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "gather": {
     "logged": 1638505431378
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "GLOBAL_SEED = 1\n",
    " \n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "GLOBAL_WORKER_ID = None\n",
    "def _init_fn(worker_id):\n",
    "    global GLOBAL_WORKER_ID\n",
    "    GLOBAL_WORKER_ID = worker_id\n",
    "    set_seed(GLOBAL_SEED + worker_id)\n",
    "\n",
    "set_seed(GLOBAL_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "gather": {
     "logged": 1638505431779
    }
   },
   "outputs": [],
   "source": [
    "def check_equal(actual, expected):\n",
    "    assert actual == expected, actual\n",
    "\n",
    "def check_approx(actual, expected):\n",
    "    assert np.allclose(actual, expected), actual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A: Project Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "gather": {
     "logged": 1638505435384
    },
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>context_paragraph</th>\n",
       "      <th>answer</th>\n",
       "      <th>answer_start</th>\n",
       "      <th>answer_end</th>\n",
       "      <th>context_sentences</th>\n",
       "      <th>answer_sent_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>When did Beyonce start becoming popular?</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "      <td>in the late 1990s</td>\n",
       "      <td>269</td>\n",
       "      <td>286</td>\n",
       "      <td>[Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What areas did Beyonce compete in when she was...</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "      <td>singing and dancing</td>\n",
       "      <td>207</td>\n",
       "      <td>226</td>\n",
       "      <td>[Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>When did Beyonce leave Destiny's Child and bec...</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "      <td>2003</td>\n",
       "      <td>526</td>\n",
       "      <td>530</td>\n",
       "      <td>[Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>In what city and state did Beyonce  grow up?</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "      <td>Houston, Texas</td>\n",
       "      <td>166</td>\n",
       "      <td>180</td>\n",
       "      <td>[Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>In which decade did Beyonce become famous?</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "      <td>late 1990s</td>\n",
       "      <td>276</td>\n",
       "      <td>286</td>\n",
       "      <td>[Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0           When did Beyonce start becoming popular?   \n",
       "1  What areas did Beyonce compete in when she was...   \n",
       "2  When did Beyonce leave Destiny's Child and bec...   \n",
       "3      In what city and state did Beyonce  grow up?    \n",
       "4         In which decade did Beyonce become famous?   \n",
       "\n",
       "                                   context_paragraph               answer  \\\n",
       "0  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...    in the late 1990s   \n",
       "1  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...  singing and dancing   \n",
       "2  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...                 2003   \n",
       "3  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...       Houston, Texas   \n",
       "4  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...           late 1990s   \n",
       "\n",
       "   answer_start  answer_end  \\\n",
       "0           269         286   \n",
       "1           207         226   \n",
       "2           526         530   \n",
       "3           166         180   \n",
       "4           276         286   \n",
       "\n",
       "                                   context_sentences  answer_sent_index  \n",
       "0  [Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ ...                  1  \n",
       "1  [Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ ...                  1  \n",
       "2  [Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ ...                  3  \n",
       "3  [Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ ...                  1  \n",
       "4  [Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ ...                  1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_squad = pd.read_csv(\n",
    "    \"cleaned_squad_data.csv\",\n",
    "    dtype = { \n",
    "        \"question\" : str,\n",
    "        \"context_paragraph\" : str,\n",
    "        \"answer\" : str,\n",
    "        \"answer_start\" : int,\n",
    "        \"answer_end\" : int,\n",
    "        \"answer_sent_index\" : int,\n",
    "        \"tokenized_context\" : str\n",
    "    },\n",
    "    converters = {\"context_sentences\" : ast.literal_eval}\n",
    ")\n",
    "\n",
    "df_squad.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the cell contents are truncated, let's print out and examine one row in detail:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "gather": {
     "logged": 1638505435965
    },
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'In what city and state did Beyonce  grow up? ',\n",
       " 'context_paragraph': 'Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny\\'s Child. Managed by her father, Mathew Knowles, the group became one of the world\\'s best-selling girl groups of all time. Their hiatus saw the release of Beyoncé\\'s debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".',\n",
       " 'answer': 'Houston, Texas',\n",
       " 'answer_start': 166,\n",
       " 'answer_end': 180,\n",
       " 'context_sentences': ['Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress.',\n",
       "  \"Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny's Child.\",\n",
       "  \"Managed by her father, Mathew Knowles, the group became one of the world's best-selling girl groups of all time.\",\n",
       "  'Their hiatus saw the release of Beyoncé\\'s debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".'],\n",
       " 'answer_sent_index': 1}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_squad.iloc[3].to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can get a better understanding of what each column means:\n",
    "* `question` is the question text.\n",
    "* `context_paragraph` is the paragraph of text that contains the answer, which our model extracts from.\n",
    "* `answer` is the ground-truth answer to the given question.\n",
    "* `answer_start` and `answer_end` are the indexes of the first and last character of `answer` within `context_paragraph`. In other words, `context_paragraph[answer_start:answer_end]` yields `answer`. Note that `answer_end` is not inclusive.\n",
    "* `context_sentences` is the list of sentences in `context_paragraph`.\n",
    "* `answer_sent_index` is the ground-truth index of the answer sentence within `context_sentences` (indexing starts from 0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several techniques to build a question-answering model from this dataset, which we will introduce in the following sections. As we implement each technique, we will try to think about its advantage and disadvantage, as compared to others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B: Unsupervised Models\n",
    "In this section, we will implement three unsupervised learning models to identify the sentence that contains the answer to a given question. Here \"unsupervised\" means that we will not make use of the ground truth answer provided in the dataset (i.e., the columns `text`, `answer_start`, and `answer_end`). Instead, the sentence identification will be based only on some pre-defined heuristics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1: Jaccard Overlap\n",
    "\n",
    "Implement the function `get_jaccard_prediction` that performs the above steps on the dataset `df_squad`. For every row, it stores the predicted answer sentence index in a new column `\"jaccard_prediction\"`, and the corresponding largest Jaccard overlap value in a new column `\"jaccard_value\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "gather": {
     "logged": 1638505442262
    }
   },
   "outputs": [],
   "source": [
    "def get_jaccard_prediction(df_squad):\n",
    "    \"\"\"\n",
    "    Identify the answer sentence as one that has the largest Jaccard overlap with the input question.\n",
    "    \n",
    "    args:\n",
    "        df_squad (pd.DataFrame) : a copy of the SQuAD dataset\n",
    "        \n",
    "    returns:\n",
    "        pd.DataFrame : the input dataframe with two additional columns, \"jaccard_prediction\" and \"jaccard_value\"\n",
    "    \"\"\"\n",
    "    jaccard_max = list()\n",
    "    y_hat = list()\n",
    "    for index, row in df_squad.iterrows():\n",
    "        q = set(nltk.tokenize.word_tokenize(row['question']))\n",
    "        jaccard_list = list()\n",
    "        for sent in row['context_sentences']:\n",
    "            sentence = set(nltk.tokenize.word_tokenize(sent))\n",
    "            if q != set() or sentence != set():\n",
    "                intersection = len(q.intersection(sentence))\n",
    "                union = (len(q) + len(sentence)) - intersection\n",
    "                jaccard = float(intersection) / union\n",
    "                jaccard_list.append(jaccard)\n",
    "            else:\n",
    "                jaccard = 1\n",
    "                jaccard_list.append(jaccard)\n",
    "        jaccard_max.append(max(jaccard_list))\n",
    "        y_hat.append(np.argmax(jaccard_list))\n",
    "        \n",
    "    df_squad['jaccard_value'] = jaccard_max\n",
    "    df_squad['jaccard_prediction'] = y_hat\n",
    "    return df_squad\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2: TF-IDF Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "gather": {
     "logged": 1638505699510
    }
   },
   "outputs": [],
   "source": [
    "def get_tfidf_prediction(df_squad, tfidf_vectorizer):\n",
    "    \"\"\"\n",
    "    Identify the answer sentence as one whose TF-IDF representation has minimal distance to that of the question.\n",
    "    \n",
    "    args:\n",
    "        df_squad (pd.DataFrame) : a copy of the SQuAD dataset\n",
    "        tfidf_vectorizer (sklearn.feature_extraction.text.TfidfVectorizer) :\n",
    "            the TF-IDF model to transform questions and sentences\n",
    "        \n",
    "    returns:\n",
    "        pd.DataFrame : the input dataframe with two additional columns, \"tfidf_prediction\" and \"distance_value\"\n",
    "    \"\"\"\n",
    "\n",
    "    df_squad_temp = df_squad.copy()\n",
    "    df_squad_temp['index'] = df_squad_temp.index.copy()\n",
    "    df_squad_temp = df_squad_temp.explode('context_sentences', True)\n",
    "    group_index = df_squad_temp.groupby('index')\n",
    "    tfidf_question = tfidf_vectorizer.transform(df_squad_temp['question'])\n",
    "    tfidf_context = tfidf_vectorizer.transform(df_squad_temp['context_sentences'])\n",
    "    df_squad_temp['distance_value'] = sp.linalg.norm(tfidf_question - tfidf_context, axis = 1)\n",
    "    index_min = group_index['distance_value'].idxmin()\n",
    "    df_squad_temp['tfidf_prediction'] = group_index.cumcount()\n",
    "    df_squad_temp = df_squad_temp.iloc[index_min].drop('index', axis = 1)\n",
    "    \n",
    "    return df_squad_temp\n",
    "    \n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an example trained TF-IDF vectorizer that we can use. Since fitting it on the entire dataset takes a while, we will initialiize and fit it in the global namespace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "gather": {
     "logged": 1638505723176
    },
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py38/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
      "/anaconda/envs/azureml_py38/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'sha', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(min_df=10, ngram_range=(1, 2),\n",
       "                stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                            'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                            \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                            'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                            'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                            'itself', ...],\n",
       "                tokenizer=<function word_tokenize at 0x7fc2fdfec280>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    tokenizer = nltk.word_tokenize,\n",
    "    stop_words = stopwords.words('english'),\n",
    "    ngram_range = (1,2),\n",
    "    max_df = 1.0,\n",
    "    min_df = 10\n",
    ")\n",
    "tfidf_vectorizer.fit(df_squad[\"context_paragraph\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed!\n",
      "CPU times: user 2min 39s, sys: 342 ms, total: 2min 39s\n",
      "Wall time: 2min 39s\n"
     ]
    }
   ],
   "source": [
    "def test_get_tfidf_prediction():\n",
    "    \"\"\"Test on the first 10 rows\"\"\"\n",
    "    df_tf_idf = get_tfidf_prediction(df_squad.head(10).copy(), tfidf_vectorizer)\n",
    "    \n",
    "    check_equal(df_tf_idf.shape, (10, 9))\n",
    "    \n",
    "    check_approx(df_tf_idf[\"distance_value\"].tolist(), [\n",
    "        1.4142135623730951, 1.4142135623730951, 1.118805210307003, 1.414213562373095,\n",
    "        1.414213562373095, 1.0991965705062294, 1.2823059131390453, 1.1299491754496973,\n",
    "        1.3217983153692023, 1.1364153055563095\n",
    "    ])\n",
    "    \n",
    "    \"\"\"Test on the whole dataset\"\"\"\n",
    "    df_tf_idf = get_tfidf_prediction(df_squad.copy(), tfidf_vectorizer)\n",
    "    \n",
    "    check_equal(df_tf_idf.shape, (86821, 9))\n",
    "    \n",
    "    check_approx(df_tf_idf.tail(10)[\"distance_value\"].tolist(), [\n",
    "        1.2205482532513834, 1.1011579092263701, 1.23935316383152, 1.2848863907388077,\n",
    "        1.1456258185112482, 1.2657059224645815, 1.4142135623730951, 1.2768968376533885,\n",
    "        1.2666294346869091, 1.4142135623730951\n",
    "    ])\n",
    "    \n",
    "    tfidf_accuracy = (df_tf_idf[\"tfidf_prediction\"] == df_tf_idf[\"answer_sent_index\"]).values.mean()\n",
    "    assert 0.68 <= round(tfidf_accuracy, 2) <= 0.69, tfidf_accuracy\n",
    "    print(\"All tests passed!\")\n",
    "    \n",
    "%time test_get_tfidf_prediction()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your implementation is sufficiently optimized, you should expect to see the local test being finished in about 3.5 minutes or less, on a `Standard_NC6` GPU Compute. If your code runs for more than 7 minutes, you should try to improve its efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 768)\n",
      "[[-0.21486233  0.39572254  0.4690868  ... -0.23119026 -0.4957911\n",
      "   0.42366332]\n",
      " [-0.44001725 -0.28488395  0.23363806 ...  0.11956061 -0.1653023\n",
      "  -0.08625244]\n",
      " [-0.29504886 -0.24928886 -0.02407114 ...  0.11944491  0.00626688\n",
      "   1.0400687 ]]\n"
     ]
    }
   ],
   "source": [
    "sent_transformer = SentenceTransformer('distilbert-base-nli-stsb-mean-tokens')\n",
    "embedding = sent_transformer.encode([\n",
    "    'This framework generates embeddings for each input sentence',\n",
    "    'Sentences are passed as a list of string.', \n",
    "    'The quick brown fox jumps over the lazy dog.'\n",
    "])\n",
    "print(embedding.shape)\n",
    "print(embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3: Sent2vec Encoders\n",
    "Here we will follow roughly the same procedure as in the previous question: first convert the questions and context sentences into vectors using `sent_transformer`, then identify the context sentence whose vector representation is closest in Euclidean distance to that of the input question. However, one important caveat is that encoding the questions and context sentences with `sent_transformer` takes a lot longer (as much as 10 times longer) than with `tfidf_vectorizer`, because this encoding is actually the forward pass through a pre-trained neural network. To address this issue, we recommend the following outline for your implementation:\n",
    "\n",
    "1. Construct a mapping from each unique question / context sentence to its vector encoding.\n",
    "1. Use this mapping to compute the vector representation of every question / context sentence in the dataset.\n",
    "1. Compute the Euclidean distances between the questions and context sentences to identify the answer sentence index for each question.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sent2vec_prediction(df_squad, encoder):\n",
    "    \"\"\"\n",
    "    Identify the answer sentence as one whose Sent2vec representation has minimal distance to that of the question.\n",
    "    \n",
    "    args:\n",
    "        df_squad (pd.DataFrame) : a copy of the SQuAD dataset\n",
    "        encoder (SentenceTransformer) :\n",
    "            the Sent2vec encoder used to transform questions and sentences into their word embeddings\n",
    "        \n",
    "    returns: Tuple(question_embeddings, context_embeddings, df_sent2vec)\n",
    "        question_embeddings (Dict[str, np.ndarray]) :\n",
    "            a mapping between each unique question and its Sent2vec embedding\n",
    "        context_embeddings (Dict[str, np.ndarray]) :\n",
    "            a mapping between each unique context sentence and its Sent2vec embedding\n",
    "        df_sent2vec (pd.DataFrame) :\n",
    "            the input dataframe with two additional columns, \"sent2vec_prediction\" and \"distance_value\"\n",
    "    \"\"\"\n",
    "        \n",
    "    df_squad_temp = df_squad.copy()\n",
    "    df_squad_temp['index'] = df_squad_temp.index.copy()\n",
    "    df_squad_temp = df_squad_temp.explode('context_sentences', True)\n",
    "    group_index = df_squad_temp.groupby('index')\n",
    "    quest_list_unique = df_squad_temp['question'].unique()\n",
    "    question_embeddings = dict(zip(quest_list_unique, encoder.encode(quest_list_unique)))\n",
    "    context_list_unique = df_squad_temp['context_sentences'].unique()\n",
    "    context_embeddings = dict(zip(context_list_unique, encoder.encode(context_list_unique)))\n",
    "    df_squad_temp['distance_value'] = df_squad_temp.progress_apply(lambda x: np.linalg.norm(question_embeddings[x['question']] - context_embeddings[x['context_sentences']]), axis = 1)\n",
    "    df_squad_temp['sent2vec_prediction'] = group_index.cumcount()\n",
    "    index_min = group_index['distance_value'].idxmin()\n",
    "    df_squad_temp = df_squad_temp.iloc[index_min].drop('index', axis = 1)\n",
    "    result = (question_embeddings, context_embeddings, df_squad_temp)\n",
    "    return result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 13667.79it/s]\n",
      "100%|██████████| 443259/443259 [00:13<00:00, 32246.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed!\n",
      "Saving the embedding to pickle files for later use ...\n",
      "Done!\n",
      "CPU times: user 6min 33s, sys: 9.16 s, total: 6min 42s\n",
      "Wall time: 6min 23s\n"
     ]
    }
   ],
   "source": [
    "def test_get_sent2vec_prediction():\n",
    "    \"\"\"Test on the first 10 rows\"\"\"\n",
    "    question_embeddings_map, context_embeddings_map, df_sent2vec = \\\n",
    "        get_sent2vec_prediction(df_squad.head(10).copy(), sent_transformer)\n",
    "    \n",
    "    question = 'When did Beyoncé rise to fame?'\n",
    "    check_approx(\n",
    "        question_embeddings_map[question][:10],\n",
    "        [-0.32787466, -0.1555715 ,  0.6588354 , -0.6630659 ,  0.58848995,\n",
    "         -0.04990903,  0.49315828,  0.24733946, -0.2781973 ,  0.60307765]\n",
    "    )\n",
    "    \n",
    "    question = 'In what city and state did Beyonce  grow up? '\n",
    "    check_approx(\n",
    "        question_embeddings_map[question][:10],\n",
    "        [-0.01007791, -0.37632802,  0.93960756, -0.5805886 ,  0.1082769 ,\n",
    "        0.25717542,  0.61284965,  0.5031233 , -0.44185746,  0.4174885 ]\n",
    "    )\n",
    "    \n",
    "    context = \"Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny's Child.\"\n",
    "    check_approx(\n",
    "        context_embeddings_map[context][:10],\n",
    "        [-0.10903959, -0.27940086,  0.23572135,  0.5180772 ,  0.35532874,\n",
    "        0.13151167,  0.17776611, -0.4766906 , -0.09587531,  0.93431026]\n",
    "    )\n",
    "    \n",
    "    context = \"Managed by her father, Mathew Knowles, the group became one of the world's best-selling girl groups of all time.\"\n",
    "    check_approx(\n",
    "        context_embeddings_map[context][:10],\n",
    "        [-0.80261433,  0.13804385,  0.59162873,  0.39244094, -0.27683446,\n",
    "        0.4693975 ,  0.41246495,  0.3003438 , -1.0914499 , -0.17863718]\n",
    "    )\n",
    "    \n",
    "    check_approx(\n",
    "        df_sent2vec[\"distance_value\"],\n",
    "        [12.849678993225098, 13.180706977844238, 11.950626373291016, 13.0397310256958,\n",
    "         12.867681503295898, 13.494369506835938, 15.975933074951172, 17.264522552490234,\n",
    "         12.373723030090332, 12.654834747314453]\n",
    "    )\n",
    "    \n",
    "    \"\"\"Test on the full dataset\"\"\"\n",
    "    question_embeddings_map, context_embeddings_map, df_sent2vec = \\\n",
    "        get_sent2vec_prediction(df_squad.copy(), sent_transformer)\n",
    "    \n",
    "    question = 'What is KMC an initialism of?'\n",
    "    check_approx(\n",
    "        question_embeddings_map[question][:10],\n",
    "        [-0.9376349 ,  0.07794607, -0.08829201,  0.23485802,  0.05154163,\n",
    "         -0.11316115, -0.15181658,  0.69910896,  0.43863776, -0.5214241]\n",
    "    )\n",
    "    \n",
    "    question = 'In what year did Kathmandu create its initial international relationship?'\n",
    "    check_approx(\n",
    "        question_embeddings_map[question][:10],\n",
    "        [0.20806803,  0.5848249 ,  0.51129144, -0.8319231 ,  0.10700534,\n",
    "        0.35735554,  0.4291537 ,  1.0774763 , -0.12320331,  0.5771949 ]\n",
    "    )\n",
    "    \n",
    "    context = \"KMC's first international relationship was established in 1975 with the city of Eugene, Oregon, United States.\"\n",
    "    check_approx(\n",
    "        context_embeddings_map[context][:10],\n",
    "        [0.65290284,  0.1876768 ,  0.5210961 , -0.13445881, -0.05164678,\n",
    "        0.5333182 ,  0.6115602 ,  0.6255963 ,  0.38067245, -0.10421762]\n",
    "    )\n",
    "    \n",
    "    context = 'It was established in 1972 and started to impart medical education from 1978.'\n",
    "    check_approx(\n",
    "        context_embeddings_map[context][:10],\n",
    "        [0.72947043, -0.5591796 ,  0.86937994, -0.80692345, -0.05610372,\n",
    "        0.07088739,  1.1165347 ,  0.8680078 ,  0.04112893, -0.71701765]\n",
    "    )\n",
    "    check_approx(\n",
    "        df_sent2vec[\"distance_value\"].tail(10),\n",
    "        [11.888211250305176, 13.456267356872559, 14.015336990356445, 14.333199501037598,\n",
    "         11.331104278564453, 10.903414726257324, 17.290882110595703, 14.260327339172363,\n",
    "         13.246850967407227, 16.56328582763672]\n",
    "    )\n",
    "    \n",
    "    sent2vec_accuracy = (df_sent2vec[\"sent2vec_prediction\"] == df_sent2vec[\"answer_sent_index\"]).mean()\n",
    "    check_equal(round(sent2vec_accuracy, 2), 0.68)\n",
    "    print(\"All tests passed!\")\n",
    "    \n",
    "    print(\"Saving the embedding to pickle files for later use ...\")\n",
    "    with open(\"question_embeddings_map.pkl\", \"wb\") as f1, open(\"context_embeddings_map.pkl\", \"wb\") as f2:\n",
    "        pickle.dump(question_embeddings_map, f1)\n",
    "        pickle.dump(context_embeddings_map, f2)\n",
    "    print(\"Done!\")\n",
    "        \n",
    "    \n",
    "%time test_get_sent2vec_prediction()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your implementation is sufficiently optimized, you should expect to see the local test finished in about 7.5 minutes or less, on a `Standard_NC6` GPU Compute. If your code runs for more than 10 minutes, you should try to improve its efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see a slight improvement in accuracy, compared to the previous two methods. It seems like using the meanings of the words isn't too effective here. Now we will try a different technique that utilizes the linguistic structures of the questions and context sentences. Let's walk through an example of what we will do first.\n",
    "\n",
    "Assume we have the following input `question`:\n",
    "\n",
    "```\n",
    "How many parameters does BERT-large have?\n",
    "```\n",
    "and `context`:\n",
    "```\n",
    "BERT-large is really big... it has 24-layers and an embedding size of 1,024, for a total of 340M parameters! Altogether it is 1.34GB, so expect it to take a couple minutes to download to your Colab instance.\n",
    "```\n",
    "\n",
    "We will perform the following steps:\n",
    "1. Lowercase `question` and identify its *root word*:\n",
    "```py\n",
    "question_root = \"have\"\n",
    "```\n",
    "1. Lemmatize this root word to reduce it to its base form.\n",
    "```py\n",
    "lemmatized_question_root = \"have\"\n",
    "```\n",
    "1. Split `context` into context sentences and lowercase them:\n",
    "```py\n",
    "sent1 = \"bert-large is really big... it has 24-layers and an embedding size of 1,024, for a total of 340m parameters!\"\n",
    "sent2 = \"altogether it is 1.34gb, so expect it to take a couple minutes to download to your colab instance.\"\n",
    "```\n",
    "1. Identify the *noun chunks* in each context sentence:\n",
    "```py\n",
    "sent1_ncs = [\"it\", \"24-layers\", \"an embedding size\", \"a total\", \"340m\", \"parameters\"]\n",
    "sent2_ncs = [\"it\", \"1.34gb\", \"it\", \"a couple minutes\", \"your colab instance\"]\n",
    "```\n",
    "1. Extract the *root word* from each noun chunk:\n",
    "```py\n",
    "sent1_nc_roots = [\"it\", \"layers\", \"size\", \"total\", \"m\", \"parameters\"]\n",
    "sent2_nc_roots = [\"it\", \"gb\", \"it\", \"minutes\", \"instance\"]\n",
    "```\n",
    "1. Identify and lemmatize the *head* for each of the above root words. Store these heads into sets:\n",
    "```py\n",
    "sent1_nc_root_heads = {\"have\", \"for\", \"layer\", \"of\"}\n",
    "sent2_nc_root_heads = {\"is\", \"take\", \"to\"}\n",
    "```\n",
    "1. Return the index of the first context sentence whose `nc_root_heads` set contains the question's root word `lemmatized_question_root`. If no context sentence meets this requirement, return 0 (in other words, we predict that the first context sentence is the answer, based on the assumption that the first sentence in a paragraph typically contains the most important information)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first define some global variables that will be employed in this task. In particular, we will use the `en_core_web_sm` model from spaCy, and the part-of-speech lemmatization procedure from Project 4. Note: the autograder will use this cell so do not change its content and do not add the tag `excluded_from_script`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_nlp = spacy.load(\"en_core_web_sm\")\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "pos_mapping = {\n",
    "    'ADJ' : wordnet.ADJ, 'NOUN' : wordnet.NOUN,\n",
    "    'VERB' : wordnet.VERB, 'ADP' : wordnet.ADV\n",
    "}\n",
    "\n",
    "def lemmatize_token(token):\n",
    "    \"\"\"\n",
    "    Lemmatize a spaCy token based on its part-of-speech tag. If a tag is not recognized, treat it as a noun.\n",
    "    \n",
    "    args:\n",
    "        token (spacy.tokens.token.Token) : an output token when inputting a raw string to a spaCy model\n",
    "    \n",
    "    return:\n",
    "        str : the lemmatized string\n",
    "    \"\"\"\n",
    "    return lemmatizer.lemmatize(token.text, pos = pos_mapping.get(token.pos_, wordnet.NOUN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [],
   "source": [
    "question = \"How many parameters does BERT-large have?\"\n",
    "context_sentences = [\n",
    "    \"BERT-large is really big... it has 24-layers and an embedding size of 1,024, for a total of 340M parameters!\",\n",
    "    \"Altogether it is 1.34GB, so expect it to take a couple minutes to download to your Colab instance.\"\n",
    "]\n",
    "\n",
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4: Root word matching\n",
    "Implement the function `get_rootword_prediction` that performs the above steps on the dataset `df_squad`. For every row, it stores:\n",
    "* the predicted answer sentence index in a new column `\"rootword_prediction\"`\n",
    "* the lemmatized root word of the question in a new column `\"question_root\"`\n",
    "* the set of lemmatized heads of the root words for the noun chunks in the predicted context sentence (for the above example, this would be the set `sent1_nc_root_heads`) in a new column `\"nc_root_heads\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rootword_prediction(df_squad, en_nlp):\n",
    "    \"\"\"\n",
    "    Identify the answer sentence as the first sentence whose list of heads of the root words for its noun chunks\n",
    "    contains the question's root word.\n",
    "    \n",
    "    args:\n",
    "        df_squad (pd.DataFrame) : a copy of the SQuAD dataset\n",
    "        en_nlp (spacy.lang.en.English) : a pre-trained SpaCy language model\n",
    "    \n",
    "    returns:\n",
    "        pd.DataFrame : the input dataframe with three additional columns,\n",
    "            \"rootword_prediction\", \"question_root\" and \"nc_root_heads\"\n",
    "    \"\"\"\n",
    "    \n",
    "    def map_quest(df_squad):\n",
    "        map_quest_root = dict()\n",
    "        unique_question = df_squad['question'].unique()\n",
    "        for unique in unique_question:\n",
    "            for sent in en_nlp(unique.lower()).sents:\n",
    "                map_quest_root[unique] = lemmatize_token(sent.root)\n",
    "                break\n",
    "        return map_quest_root\n",
    "    \n",
    "    def rootword_pred(sent1, sent2):\n",
    "        chunk_head = {lemmatize_token(chunk.root.head) for chunk in en_nlp(sent2[0].lower()).noun_chunks}\n",
    "        for index, sent in enumerate(sent2):\n",
    "            chunk_head_ = set()\n",
    "            for chunk in en_nlp(sent.lower()).noun_chunks:\n",
    "                chunk_head_.add(lemmatize_token(chunk.root.head))\n",
    "            if map_quest_root[sent1] in chunk_head_:\n",
    "                return (index, chunk_head_)\n",
    "        return (0, chunk_head)\n",
    "    \n",
    "    df_squad_temp = df_squad.copy()\n",
    "    map_quest_root = map_quest(df_squad_temp)\n",
    "    \n",
    "    df_squad_temp[['rootword_prediction', 'nc_root_heads']] = df_squad_temp.progress_apply(lambda x: rootword_pred(x['question'], x['context_sentences']), result_type = 'expand', axis = 1)\n",
    "    df_squad_temp['question_root'] = df_squad_temp['question'].map(map_quest_root)\n",
    "\n",
    "    return df_squad_temp   \n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part C: Supervised Models\n",
    "So far we have been exploring unsupervised methods for answer extraction which involves dividing the questions and contexts into tokens and projecting those tokens into a common representation space. You may notice that their performances weren't particularly great because we didn't perform any training on the dataset; instead, we only used pre-defined heuristics and pre-trained models. From this point, we will move to the supervised learning domain, where we make use of the ground-truth answers and build models that learn from these answers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5: Preparing dataset for supervised learning\n",
    "We will first consider a binary classification setting, where we are given a question and a context sentence, and need to predict whether this context sentence contains the answer. In this setting, we can get several training data points from each row in the original dataset `df_squad`. In particular, if a row in `df_squad` looks like the following:\n",
    "\n",
    "|question|context_sentences|answer_sent_index|\n",
    "|---|---|---|\n",
    "|`q`|`[s0, s1, s2, s3]`|2|\n",
    "\n",
    "then it contributes four data points:\n",
    "\n",
    "|question|context_sentence|is_answer_sent|\n",
    "|---|---|---|\n",
    "|`q`|`s0`|0|\n",
    "|`q`|`s1`|0|\n",
    "|`q`|`s2`|1|\n",
    "|`q`|`s3`|0|\n",
    "\n",
    "More generally, a row in `df_squad` where the `context_sentences` list has `n` sentences will be transformed into `n` rows, one for each context sentence. Among these new rows, only the row at index `answer_sent_index` gets assigned the label 1, while the others get the label 0.\n",
    "\n",
    "Implement the function `build_data_for_classification` that turns the original dataset `df_squad` into a dataframe with 3 columns -- `question`, `context_sentence` and `is_answer_sent` -- using the procedure specified above.\n",
    "\n",
    "**Notes**:\n",
    "* Keep in mind that the new dataframe has a column named `context_sentence`, **not** `context_sentences`.\n",
    "* You should preserve the original row ordering in the input dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_data_for_classification(df_squad):\n",
    "    \"\"\"\n",
    "    Convert the SQuAD dataset into a format where every row contains one question, one context answer,\n",
    "    and a flag that indicates whether the context sentence is the answer.\n",
    "    \n",
    "    args:\n",
    "        df_squad (pd.DataFrame) : a copy of the SQuAD dataset\n",
    "        \n",
    "    returns:\n",
    "        pd.DataFrame : a new dataframe with 3 columns: question, context_sentence, is_answer_sent\n",
    "    \"\"\"\n",
    "    df_squad_temp = df_squad.copy()\n",
    "    df_squad_temp['index'] = df_squad_temp.index.copy()\n",
    "    df_squad_temp = df_squad_temp.rename({'context_sentences': 'context_sentence'}, axis = 1)\n",
    "    df_squad_temp = df_squad_temp.explode('context_sentence', True)\n",
    "    group_index = df_squad_temp.groupby(['index'])\n",
    "    df_squad_temp['is_answer_sent'] = group_index.cumcount()\n",
    "    df_squad_temp['is_answer_sent'] = df_squad_temp.progress_apply(lambda x: 1 if x['answer_sent_index'] == x['is_answer_sent'] else 0, axis = 1)\n",
    "    result_df = df_squad_temp[['question', 'context_sentence', 'is_answer_sent']]\n",
    "    return result_df\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:00<00:00, 24612.05it/s]\n",
      "100%|██████████| 443259/443259 [00:05<00:00, 76549.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def test_build_data_for_classification():\n",
    "    \"\"\"Test on 10 random rows\"\"\"\n",
    "    df_sample = df_squad.sample(n = 10, random_state = 0).copy()\n",
    "    df_formatted = build_data_for_classification(df_sample)\n",
    "    \n",
    "    assert df_formatted.shape == (48, 3), df_formatted.shape\n",
    "    \n",
    "    assert sorted(df_formatted.columns) == ['context_sentence', 'is_answer_sent', 'question']\n",
    "    \n",
    "    assert df_formatted['is_answer_sent'].tolist() == [\n",
    "        0, 0, 1, 1, 0, 0, 0, 0, 0, 0,\n",
    "        0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
    "        0, 1, 0, 0, 0, 0, 0, 0, 0, 1,\n",
    "        0, 0, 1, 0, 0, 0, 0, 1, 0, 0,\n",
    "        1, 0, 0, 0, 1, 0, 1, 0\n",
    "    ], df_formatted['is_answer_sent'].tolist()\n",
    "    \n",
    "    # check that question orderings are preserved\n",
    "    assert (df_formatted[\"question\"].unique() == df_sample[\"question\"].unique()).all()\n",
    "    \n",
    "    \"\"\"Test on the full dataset\"\"\"\n",
    "    df_formatted = build_data_for_classification(df_squad.copy())\n",
    "    \n",
    "    assert df_formatted.shape == (443259, 3), df_formatted.shape\n",
    "    \n",
    "    assert df_formatted['is_answer_sent'].sample(n = 40, random_state = 200).tolist() == [\n",
    "        1, 0, 0, 1, 0, 0, 1, 0, 0, 0,\n",
    "        0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "        0, 0, 1, 0, 0, 0, 0, 1, 0, 1\n",
    "    ], df_formatted['is_answer_sent'].tail(40).tolist()\n",
    "    \n",
    "    assert (df_formatted[\"question\"].unique() == df_squad[\"question\"].unique()).all()\n",
    "    print(\"All tests passed!\")\n",
    "    \n",
    "test_build_data_for_classification()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before building our supervised learning models, we will load the embeddings you created in Question 3 and set up the train set and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [],
   "source": [
    "question_embeddings_map = pd.read_pickle(\"question_embeddings_map.pkl\")\n",
    "context_embeddings_map = pd.read_pickle(\"context_embeddings_map.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 354244/354244 [00:04<00:00, 74451.13it/s]\n",
      "100%|██████████| 89015/89015 [00:01<00:00, 74844.85it/s]\n"
     ]
    }
   ],
   "source": [
    "df_squad_train, df_squad_test = train_test_split(df_squad, train_size = 0.8, random_state = 0)\n",
    "df_train_formatted = build_data_for_classification(df_squad_train.reset_index())\n",
    "df_test_formatted = build_data_for_classification(df_squad_test.reset_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6: Logistic Regression\n",
    "Having set up the dataset for binary classification, we can now train a logistic regression model. While the binary labels are already set up, we still need to construct the feature vectors as follows. For every question `q` and context sentence `s`:\n",
    "* Convert the question to its word embedding $x_q \\in \\mathbb{R}^k$, using the question embedding map from Question 3.\n",
    "* Convert the context sentence to its word embedding $x_s \\in \\mathbb{R}^l$, using the context embedding map from Question 3.\n",
    "* Concatenate these two vectors to get the input vector to the logistic regression model:\n",
    "\n",
    "$$x_{q,s} = (x_{q1} \\quad x_{q2} \\quad \\ldots \\quad x_{qk} \\quad x_{s1} \\quad x_{s2} \\quad \\ldots \\quad x_{sl})^T \\in \\mathbb{R}^{k+l}.$$\n",
    "\n",
    "Implement the function `get_lr_prediction` that performs the following steps:\n",
    "\n",
    "1. Construct the feature vector for each row of the train set `df_train_formatted`, using the above formula.\n",
    "1. Use an Sklearn `StandardScaler` (with default parameters) to fit and transform the train set `df_train_formatted`.\n",
    "1. Train an Sklearn `LogisticRegression` model on the train set `df_train_formatted`.\n",
    "1. Use this model to perform prediction on the test set `df_test_formatted`.\n",
    "1. Return the trained LR model and its accuracy on the test set (i.e., the number of correct predictions divided by the test set size).\n",
    "\n",
    "**Notes**:\n",
    "* When creating a `LogisticRegression` model you should set `random_state` to the input `seed` and `max_iters` to 1000. You do not need to specify any other parameter.\n",
    "* Make sure you also normalize the feature matrix built from the test set before inputting it to the LR model for prediction.\n",
    "* Be careful when dealing with nested NumPy arrays. If you get a NumPy array that contains other NumPy arrays, you can use `np.stack` to turn it to a normal multi-dimensional array (otherwise it will be treated as a 1D array of pointers to other arrays)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr_prediction(df_train_formatted, df_test_formatted, question_embeddings_map, context_embeddings_map, seed = 0):\n",
    "    \"\"\"\n",
    "    Train and evaluate the performance of a binary logisitic regression model to predict\n",
    "    whether a context sentence contains the answer to a given question.\n",
    "    \n",
    "    args:\n",
    "        df_train_formatted (pd.DataFrame) : the train set dataframe with 3 columns:\n",
    "            question, context_sentence, is_answer_sent\n",
    "        df_test_formatted (pd.DataFrame) : the test set dataframe with 3 columns:\n",
    "            question, context_sentence, is_answer_sent\n",
    "        question_embeddings_map (dict[str, np.ndarray]) : a mapping from question to word embedding\n",
    "        context_embeddings_map (dict[str, np.ndarray]) : a mapping from context sentence to word embedding\n",
    "        seed (int) : the random generator used in LogisticRegression\n",
    "        \n",
    "    return: Tuple(trained_model, accuracy)\n",
    "        trained_model (sklearn.linear_model.LogisticRegression) : the LR model trained on the train set\n",
    "        accuracy (float) : the accuracy score of the trained model on the test set\n",
    "    \"\"\"\n",
    "    \n",
    "    logst_reg = LogisticRegression(random_state = seed, max_iter = 1000)\n",
    "    std_scaler = StandardScaler()\n",
    "    df_train_formatted['question'] = df_train_formatted['question'].map(question_embeddings_map)\n",
    "    df_train_formatted['context_sentence'] = df_train_formatted['context_sentence'].map(context_embeddings_map)\n",
    "    df_test_formatted['question'] = df_test_formatted['question'].map(question_embeddings_map)\n",
    "    df_test_formatted['context_sentence'] = df_test_formatted['context_sentence'].map(context_embeddings_map)\n",
    "    df_train_formatted['concat'] = df_train_formatted.progress_apply(lambda x: np.concatenate((x['question'], x['context_sentence'])), axis = 1)\n",
    "    train = std_scaler.fit_transform(np.stack(df_train_formatted['concat']))\n",
    "    logst_reg.fit(train, df_train_formatted['is_answer_sent'])\n",
    "    df_test_formatted['concat'] = df_test_formatted.progress_apply(lambda x: np.concatenate((x['question'], x['context_sentence'])), axis = 1)\n",
    "    test = std_scaler.transform(np.stack(df_test_formatted['concat']))\n",
    "    prediction = logst_reg.predict(test)\n",
    "    accuracy = (prediction == df_test_formatted['is_answer_sent']).mean()\n",
    "    result = (logst_reg, accuracy)\n",
    "    return result\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4666/4666 [00:00<00:00, 73641.87it/s]\n",
      "100%|██████████| 1107/1107 [00:00<00:00, 72419.36it/s]\n",
      "100%|██████████| 4666/4666 [00:00<00:00, 50681.53it/s]\n",
      "100%|██████████| 1107/1107 [00:00<00:00, 44805.84it/s]\n",
      "100%|██████████| 354244/354244 [00:08<00:00, 41235.56it/s]\n",
      "100%|██████████| 89015/89015 [00:02<00:00, 42134.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed!\n"
     ]
    }
   ],
   "source": [
    "def test_get_lr_prediction():\n",
    "    \"Test on the first 1000 rows of the dataset\"\n",
    "    df_squad_train_mini, df_squad_test_mini = train_test_split(df_squad.head(1000), train_size = 0.8, random_state = 0)\n",
    "    df_train_formatted_mini = build_data_for_classification(df_squad_train_mini.reset_index())\n",
    "    df_test_formatted_mini = build_data_for_classification(df_squad_test_mini.reset_index())\n",
    "    lr_mini, acc_mini = get_lr_prediction(\n",
    "        df_train_formatted_mini, df_test_formatted_mini,\n",
    "        question_embeddings_map, context_embeddings_map\n",
    "    )\n",
    "    assert lr_mini.coef_.flatten()[:10].round(2).tolist() == [\n",
    "        0.07, -0.01, -0.03, -0.03, -0.01, 0.01, -0.03, 0.0, 0.04, -0.04\n",
    "    ]\n",
    "    assert lr_mini.intercept_.round(2)[0] == -2.73\n",
    "    assert round(acc_mini, 2) ==  0.81\n",
    "    \n",
    "    \"\"\"Test on the entire dataset\"\"\"\n",
    "    lr, acc = get_lr_prediction(df_train_formatted, df_test_formatted, question_embeddings_map, context_embeddings_map)\n",
    "    assert lr.coef_.flatten()[:10].round(2).tolist() == [\n",
    "        -0.04, 0.0, 0.01, 0.0, -0.0, 0.0, -0.01, -0.0, -0.03, 0.01\n",
    "    ]\n",
    "    assert lr.intercept_.round(2)[0] == -1.54\n",
    "    assert round(acc, 2) == 0.80\n",
    "    \n",
    "    print(\"All tests passed!\")\n",
    "    \n",
    "test_get_lr_prediction()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtain about 80% accuracy in this binary classification task, which is not too bad! However, it's important to note that this accuracy cannot be compared with those from previous questions, because it is evaluated in a different setting (`df_test_formatted`). If we were only interested in whether the ground truth answer sentences are correctly detected, we would evaluate the accuracy on the original test set `df_squad_test`, instead of the formatted one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While logistic regression on Sent2vec representation performs relatively well, it still relies on a *uni-directional* representation of words. In this setting, the same word is always mapped to the same vector, even though it may have different meanings in different contexts (e.g., the word `bank` in `bank account` is not the same as in `river bank`). The final model we will explore in this project, which also addresses the above issue, is called BERT (Bi-directional Encoder Representations from Transformers). This is a language model that learns to predict the probability of a sequence of words. The reason for BERT's success is its large feedforward layers and its attention heads, giving it 110 million parameters for the base model and 340 million parameters for the large model. It has been trained on Wikipedia articles and the Book Corpus dataset, which contains text from over 10,000 books of different genres over the tasks of next sentence prediction (NSP) and masked language modeling (MLM). BERT represents the current state of the art in various NLP task, including question answering.\n",
    "\n",
    "One nice feature of NLP models like BERT is that they have already been pre-trained on massive text corpuses, but can also be fine-tuned further for a specific domain (in this case, our SQuAD dataset). We will implement this workflow in the rest of the project. First, we define a sub-class of `Dataset`, similar to Project 6, to turn our SQuaD dataset into a format that PyTorch can work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SQuADDataset(Dataset):\n",
    "    def __init__(self, df_squad):\n",
    "        \"\"\"\n",
    "        Class constructor.\n",
    "        \"\"\"\n",
    "        self.data = df_squad\n",
    "        self.data_cols = [\"question\", \"context_paragraph\", \"answer_start\", \"answer_end\"]\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Get the dataset length.\n",
    "        \"\"\"\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Get the question, context paragraph, answer start and answer end value\n",
    "        at the row specified by the input index from the dataset.\n",
    "        \"\"\"\n",
    "        return tuple(self.data.loc[index, self.data_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7: Tokenization for BERT\n",
    "Similar to how our `LogisticRegression` model expects a real-valued vector as input, Bert also has its own way of constructing the input. At a high level, we want to convert each input tuple\n",
    "```\n",
    "(question, context_paragraph, answer_start, answer_end)\n",
    "```\n",
    "into a tuple\n",
    "```\n",
    "(encoding, token_start, token_end)\n",
    "```\n",
    "where `encoding` is a dictionary that maps three keywords -- `\"input_ids\"`, `\"token_type_ids\"`, `\"attention_mask\"` -- to their respective vector representations.\n",
    "\n",
    "Implement the class `SQuADTokenizer` that performs the above conversion using a pre-trained Bert tokenizer. In particular, the class constructor accepts a `BertTokenizer` instance and the maximum sequence size `max_length`, which are stored as instance variables. Then, the `__call__` function acccepts a batch of data and performs the following steps:\n",
    "1. Extract the questions, context paragraphs, answer start indexes, and answer end indexes from the batch.\n",
    "1. Input the questions and contexts to the tokenizer as separate lists, so that the tokenizer can append special tokens such as `[SEP]` that help BERT recognize different passages. You should also set the following parameters: `padding` to `\"longest\"`, `truncation` to `True`, `max_length` to the stored `max_len`, and `return_tensors` to `\"pt\"`.\n",
    "1. Convert the `answer_start` and `answer_end` character indices to the indices of the two tokens that correspond to these start and end characters. For example, let's say:\n",
    "```py\n",
    "question = \"Which pets does your son have?\"\n",
    "context_paragraph = \"My son loves pets. He has two cats and a dog.\"\n",
    "tokenized_context = [\"my\", \"son\", \"love\", \"pet\", \"he\", \"have\", \"two\", \"cat\", \"and\", \"a\", \"dog\"]\n",
    "answer_start, answer_end = 26, 44 # answer = \"two cats and a dog\"\n",
    "```\n",
    "now you need to identify the token that corresponds to the character `context_paragraph[answer_start]`. This character is `t` and the corresponding token is `two`, which is at index `6` in `tokenized_context`. Similarly for `answer_end`, the character `context_paragraph[answer_end]` is `g`, and the corresponding token is `dog`, which is at index 10 in `tokenized_context`. In summary, you are converting\n",
    "```\n",
    "(answer_start = 26, answer_end = 44)\n",
    "```\n",
    "to\n",
    "```\n",
    "(token_start = 6, token_end = 10)\n",
    "```\n",
    "1. Return the encoded data (output from calling the tokenizer on the input questions and context paragraphs in Step 2), as well as the list of token start indexes and the list of token end indexes.\n",
    "\n",
    "**Notes**:\n",
    "* The [Tokenizer page](https://huggingface.co/transformers/main_classes/tokenizer.html) and the section about [using the tokenizer](https://huggingface.co/transformers/quicktour.html?highlight=max_length) on HuggingFace may be helpful.\n",
    "* To convert `(answer_start, answer_end)` to `(token_start, token_end)`, you can use the [.char_to_token](https://huggingface.co/transformers/main_classes/tokenizer.html#transformers.BatchEncoding.char_to_token) method. Here the `batch_or_char_index` is the index of the current data point within the input batch, and `sequence_index` should be 1 because we are doing this conversion on the context paragraph, which is the second input string to the tokenizer.\n",
    "* Sometimes calling `.char_to_token` will return `None` because the tokens have been changed (e.g., due to lemmatization) from their original form in `context_paragraph`. A simple work-around is to also consider the token that corresponds to the next or previous character. If doing so still yields `None`, we will just set the token index as `max_len` as a last resort. More formally, you should assign `token_start` to the first element that is not `None` in the following list:\n",
    "```\n",
    "[\n",
    "    char_to_token(char_index = answer_start, ...),\n",
    "    char_to_token(char_index = answer_start+1, ...),\n",
    "    char_to_token(char_index = answer_start-1, ...),\n",
    "    max_len\n",
    "]\n",
    "```\n",
    "and do a similar assignment for `token_end`.\n",
    "* When extracting `answer_starts` and `anwer_ends` from the batch, they will have the type `torch.Tensor` (i.e., these are float tensors). You should convert them to `LongTensor` so that they can be used in `char_to_token()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SQuADTokenizer:\n",
    "    def __init__(self, tokenizer, max_len = 512):\n",
    "        \"\"\"\n",
    "        Store the input BertTokenizer instance and the max length\n",
    "        \"\"\"\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_len\n",
    "        pass\n",
    "    \n",
    "    def __call__(self, batch):\n",
    "        \"\"\"\n",
    "        Perform tokenization on a batch of data\n",
    "        \n",
    "        args:\n",
    "            batch (Tuple[questions, contexts, answer_starts, answer_ends]):\n",
    "                questions (List[str]) : a list of questions\n",
    "                contexts (List[str]) : a list of context paragraphs\n",
    "                answer_starts (List[int]) : a list of answer start indexes\n",
    "                answer_ends (List[int]) : a list of answer end indexes\n",
    "        \n",
    "        returns:\n",
    "            Tuple[encoding, token_starts, token_ends]\n",
    "                encoding (dict[str, tensor]): the output of calling tokenizer on the questions and contexts\n",
    "                token_starts (List[int]) : the list of indexes for the tokens that correspond to the first answer character\n",
    "        \"\"\"\n",
    "        \n",
    "        questions = list(batch[0])\n",
    "        contexts = list(batch[1])\n",
    "        encoding = self.tokenizer(questions, contexts, padding = \"longest\", truncation = True, max_length = self.max_length, return_tensors = 'pt')\n",
    "        \n",
    "        def get_index(indexes):\n",
    "            final = []\n",
    "            for i in range(len(indexes)):\n",
    "                if encoding.char_to_token(i, char_index = indexes[i], sequence_index = 1) != None:\n",
    "                    temp = encoding.char_to_token(i, char_index = indexes[i], sequence_index = 1)\n",
    "                    final.append(temp)\n",
    "                elif encoding.char_to_token(i, char_index = indexes[i] + 1, sequence_index = 1) != None:\n",
    "                    temp = encoding.char_to_token(i, char_index = indexes[i] + 1, sequence_index = 1)\n",
    "                    final.append(temp)\n",
    "                elif encoding.char_to_token(i, char_index = indexes[i] - 1, sequence_index = 1) != None:\n",
    "                    temp = encoding.char_to_token(i, char_index = indexes[i] - 1, sequence_index = 1)\n",
    "                    final.append(temp)\n",
    "                else:\n",
    "                    final.append(self.max_length)\n",
    "                    \n",
    "            return final\n",
    "        \n",
    "        answer_starts = torch.LongTensor(batch[2])\n",
    "        token_starts = get_index(answer_starts)\n",
    "        answer_ends = torch.LongTensor(batch[3])\n",
    "        token_ends = get_index(answer_ends)\n",
    "        \n",
    "        result = (encoding, token_starts, token_ends)\n",
    "        return result\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed!\n"
     ]
    }
   ],
   "source": [
    "def test_tokenizer():\n",
    "    tokenizer = BertTokenizerFast.from_pretrained('rsvp-ai/bertserini-bert-base-squad')\n",
    "    batch_tokenizer = SQuADTokenizer(tokenizer)\n",
    "    example_1 = (\n",
    "        ['When did Beyonce start becoming popular?'],\n",
    "        ['Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny\\'s Child. Managed by her father, Mathew Knowles, the group became one of the world\\'s best-selling girl groups of all time. Their hiatus saw the release of Beyoncé\\'s debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".'],\n",
    "        [269],\n",
    "        [286]\n",
    "    )\n",
    "    encoding, token_starts, token_ends = batch_tokenizer(example_1)\n",
    "    check_equal(list(encoding[\"input_ids\"].shape), [1, 174])\n",
    "    check_equal(encoding[\"input_ids\"][0][:10].numpy().tolist(), [\n",
    "        101, 2043, 2106, 20773, 2707, 3352, 2759, 1029, 102, 20773\n",
    "    ])\n",
    "    check_equal(encoding[\"token_type_ids\"].numpy().tolist(), [[0]*9 + [1]*165])\n",
    "    check_equal(encoding[\"attention_mask\"].numpy().tolist(), [[1] * 174])\n",
    "    check_equal(token_starts, [75])\n",
    "    check_equal(token_ends, [79])\n",
    "\n",
    "    example_2 = (\n",
    "        ['When did Beyonce start becoming popular?', 'What score did the writer from the Chicago Tribune give to Spectre?'], \n",
    "        ['Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny\\'s Child. Managed by her father, Mathew Knowles, the group became one of the world\\'s best-selling girl groups of all time. Their hiatus saw the release of Beyoncé\\'s debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".', 'Critical appraisal of the film was mixed in the United States. In a lukewarm review for RogerEbert.com, Matt Zoller Seitz gave the film 2.5 stars out of 4, describing Spectre as inconsistent and unable to capitalise on its potential. Kenneth Turan, reviewing the film for Los Angeles Times, concluded that Spectre \"comes off as exhausted and uninspired\". Manohla Dargis of The New York Times panned the film as having \"nothing surprising\" and sacrificing its originality for the sake of box office returns. Forbes\\' Scott Mendelson also heavily criticised the film, denouncing Spectre as \"the worst 007 movie in 30 years\". Darren Franich of Entertainment Weekly viewed Spectre as \"an overreaction to our current blockbuster moment\", aspiring \"to be a serialized sequel\" and proving \"itself as a Saga\". While noting that \"[n]othing that happens in Spectre holds up to even minor logical scrutiny\", he had \"come not to bury Spectre, but to weirdly praise it. Because the final act of the movie is so strange, so willfully obtuse, that it deserves extra attention.\" In a positive review Rolling Stone, Peter Travers gave the film 3.5 stars out of 4, describing \"The 24th movie about the British MI6 agent with a license to kill is party time for Bond fans, a fierce, funny, gorgeously produced valentine to the longest-running franchise in movies\". Other positive reviews from Mick LaSalle from the San Francisco Chronicle, gave it a perfect 100 score, stating: “One of the great satisfactions of Spectre is that, in addition to all the stirring action, and all the timely references to a secret organization out to steal everyone’s personal information, we get to believe in Bond as a person.” Stephen Whitty from the New York Daily News, gave it an 80 grade, saying: “Craig is cruelly efficient. Dave Bautista makes a good, Oddjob-like assassin. And while Lea Seydoux doesn’t leave a huge impression as this film’s “Bond girl,” perhaps it’s because we’ve already met — far too briefly — the hypnotic Monica Bellucci, as the first real “Bond woman” since Diana Rigg.” Richard Roeper from the Chicago Sun-Times, gave it a 75 grade. He stated: “This is the 24th Bond film and it ranks solidly in the middle of the all-time rankings, which means it’s still a slick, beautifully photographed, action-packed, international thriller with a number of wonderfully, ludicrously entertaining set pieces, a sprinkling of dry wit, myriad gorgeous women and a classic psycho-villain who is clearly out of his mind but seems to like it that way.” Michael Phillips over at the Chicago Tribune, gave it a 75 grade. He stated: “For all its workmanlike devotion to out-of-control helicopters, “Spectre” works best when everyone’s on the ground, doing his or her job, driving expensive fast cars heedlessly, detonating the occasional wisecrack, enjoying themselves and their beautiful clothes.” Guy Lodge from Variety, gave it a 70 score, stating: “What’s missing is the unexpected emotional urgency of “Skyfall,” as the film sustains its predecessor’s nostalgia kick with a less sentimental bent.”'], \n",
    "        [269, 2118], \n",
    "        [286, 2120]\n",
    "    )\n",
    "    encoding, token_starts, token_ends = batch_tokenizer(example_2)\n",
    "    check_equal(list(encoding[\"input_ids\"].shape), [2, 512])\n",
    "    check_equal(encoding[\"input_ids\"][0][-10:].numpy().tolist(), [0]*10)\n",
    "    check_equal(encoding[\"token_type_ids\"].numpy().tolist()[0], [0]*9 + [1]*165 + [0]*338)\n",
    "    check_equal(encoding[\"token_type_ids\"].numpy().tolist()[1], [0]*16 + [1]*496)\n",
    "    check_equal(encoding[\"attention_mask\"][0].numpy().tolist(), [1]*174 + [0]*338)\n",
    "    check_equal(token_starts, [75, 512])\n",
    "    check_equal(token_ends, [79, 512])\n",
    "    print(\"All tests passed!\")\n",
    "    \n",
    "test_tokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8: Fine-tuning BERT\n",
    "With the tokenizer ready, we can begin fine-tuning a pre-trained BERT model to our dataset with the following steps:\n",
    "1. Move the input `model` to `device` and set it to training mode.\n",
    "1. Repeat `n_iters` times:\n",
    "    * For every batch of data from the dataloader:\n",
    "        * Use the input `tokenizer` (an instance of the class `SQuADTokenizer` that you implemented) to encode this batch, yielding the tuple `(encoding, token_starts, token_ends)`.\n",
    "        * Extract the `input_ids`, `token_type_ids` and `attention_mask` from `encoding`. Convert these tensors to `LongTensor` and move them to `device`.\n",
    "        * Perform the usual PyTorch training workflow (zero grad, forward pass, backprop, ...). See the PyTorch primer for a reminder.\n",
    "        \n",
    "Implement the function `fine_tune_bert` that, given a pretrained BERT model and other training parameters, performs the above training procedure. This function should return the fine-tuned model and the average training loss across epochs.\n",
    "\n",
    "**Notes**:\n",
    "* To carry out the forward pass, you can input the encoding elements (after convering them to datatype Long), along with `token_starts` and `token_ends`, to `model`. Calling `.loss` on the output of the forward pass will yield the loss value.\n",
    "* We also provide a `verbose` flag. If you would like to add print debugging messages during model training, simply precede each print statement with an `if verbose` check, for example:\n",
    "```py\n",
    "if verbose:\n",
    "    print(\"Training loss\", train_loss)\n",
    "```\n",
    "The autograder will only call your function with `verbose = False`, so that your printout messages do not interfere with grading.\n",
    "* To compute the average training loss, you should sum all the losses from every forward pass, then divide this sum by the number of epochs at the end of the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune_bert(model, n_epochs, optimizer, dataloader, squad_tokenizer, device, verbose = False):\n",
    "    \"\"\"\n",
    "    Fine-tune a pre-trained BERT model on the SQuAD dataset.\n",
    "    \n",
    "    args:\n",
    "        model (BertForQuestionAnswering) : a pre-trained BERT model for QA tasks\n",
    "        n_epochs (int) : the number of epochs to train\n",
    "        dataloader (DataLoader) : a data loader that provides access to one batch of data at a time\n",
    "        squad_tokenizer (SQuADTokenizer) : a tokenizer instance to be called on every batch of data from dataloader\n",
    "        device (torch.device) : the device (CPU or Cuda) that the model and data should be moved to\n",
    "        verbose (bool) : a flag that indicates whether debug messages should be printed out\n",
    "    \n",
    "    return:\n",
    "        model (BertForQuestionAnswering) : the fine-tuned model\n",
    "        avg_loss (float) : the average training loss across epochs\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    sum_loss = 0.0\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        for batch_token in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            encoding, token_starts, token_ends = squad_tokenizer(batch_token)\n",
    "            token_starts = torch.LongTensor(token_starts).to(device)\n",
    "            token_ends = torch.LongTensor(token_ends).to(device)\n",
    "            input_ids = torch.LongTensor(encoding['input_ids']).to(device)\n",
    "            attention_mask = torch.LongTensor(encoding['attention_mask']).to(device)\n",
    "            token_type_ids = torch.LongTensor(encoding['token_type_ids']).to(device)\n",
    "             \n",
    "            loss = model(input_ids, attention_mask, token_type_ids, start_positions = token_starts,\n",
    "                         end_positions = token_ends).loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            sum_loss += loss.item()\n",
    "            \n",
    "            \n",
    "    average_loss = sum_loss/n_epochs\n",
    "    return model, average_loss \n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "All tests passed!\n",
      "CPU times: user 9.06 s, sys: 5.44 s, total: 14.5 s\n",
      "Wall time: 14.1 s\n"
     ]
    }
   ],
   "source": [
    "def test_fine_tune_bert():\n",
    "    tokenizer = BertTokenizerFast.from_pretrained('rsvp-ai/bertserini-bert-base-squad')\n",
    "    squad_tokenizer = SQuADTokenizer(tokenizer)\n",
    "    \n",
    "    \"\"\"Train on 8 data points\"\"\"\n",
    "    train_dataset = SQuADDataset(df_squad.head(8)[[\"question\", \"context_paragraph\", \"answer_start\", \"answer_end\"]])\n",
    "    squad_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=False, num_workers=6, worker_init_fn=_init_fn)\n",
    "    model = BertForQuestionAnswering.from_pretrained('rsvp-ai/bertserini-bert-base-squad')\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=2e-5, eps=1e-8)  \n",
    "    model, avg_train_loss = fine_tune_bert(model, 1, optimizer, squad_dataloader, squad_tokenizer, device)\n",
    "    assert avg_train_loss < 5.0, avg_train_loss\n",
    "    \n",
    "    \"\"\"Train on 100 data points\"\"\"\n",
    "    train_dataset = SQuADDataset(df_squad.head(100)[[\"question\", \"context_paragraph\", \"answer_start\", \"answer_end\"]])\n",
    "    squad_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=False, num_workers=6, worker_init_fn=_init_fn)\n",
    "    model = BertForQuestionAnswering.from_pretrained('rsvp-ai/bertserini-bert-base-squad')\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=2e-5, eps=1e-8)  \n",
    "    model, avg_train_loss = fine_tune_bert(model, 1, optimizer, squad_dataloader, squad_tokenizer, device)\n",
    "    assert avg_train_loss < 38, avg_train_loss\n",
    "    print(\"All tests passed!\")\n",
    "    \n",
    "%time test_fine_tune_bert()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will fine-tune the model on 80% of the data (the remaining 20% will be for evaluation). Because this takes a long time, we will only do it for 1 epoch.\n",
    "\n",
    "**Notes**:\n",
    "* The following cell will take about **2 hours** to run, and is **required** for the next question. We recommend that you make a submission to TPZ at this point, to make sure you have everything correct so far. Ideally you would want to avoid having to fine-tune Bert more than once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "CPU times: user 1h 20min 47s, sys: 37min 19s, total: 1h 58min 6s\n",
      "Wall time: 1h 57min 20s\n"
     ]
    }
   ],
   "source": [
    "model = BertForQuestionAnswering.from_pretrained('rsvp-ai/bertserini-bert-base-squad')\n",
    "tokenizer = BertTokenizerFast.from_pretrained('rsvp-ai/bertserini-bert-base-squad')\n",
    "train_indexes, test_indexes = train_test_split(df_squad.index, train_size = 0.8, random_state = 0)\n",
    "df_squad_train = df_squad.loc[train_indexes, [\"question\", \"context_paragraph\", \"answer_start\", \"answer_end\"]].reset_index()\n",
    "df_squad_test = df_squad.loc[test_indexes].reset_index()\n",
    "\n",
    "train_dataset = SQuADDataset(df_squad_train)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=6, shuffle=False, num_workers=6, worker_init_fn=_init_fn, pin_memory = True)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
    "squad_tokenizer = SQuADTokenizer(tokenizer)\n",
    "\n",
    "%time tuned_model, avg_train_loss = fine_tune_bert(model, 1, optimizer, train_dataloader, squad_tokenizer, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also save the fine-tuned model into a directory to reuse it later. Note that the following code will create a directory `bert_fine_tuned_squad` with several files in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [],
   "source": [
    "tuned_model.cpu().save_pretrained(\"bert_fine_tuned_squad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the fine tuned model performs. We provide the following function `get_bert_prediction` to predict the answer given a pair of question and context. Overall the inference process is very similar to the forward pass during fine-tuning, except that we don't provide the starting and ending tokens to the Bert model, since those are what we need to predict.\n",
    "\n",
    "You may also notice that we are looping through each data point, instead of doing inference in a vectorized manner. The reason is that this same inference code will be used for model deployment on CPU later, where we have limited memory and cannot afford to process data in batches (recall how you performed inference both in the loop approach and the batch approach in the OPE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_prediction(questions, contexts, device, model=None, tokenizer=None):\n",
    "    '''\n",
    "    Given list of questions and list of corresponding contexts predicts answers using BERT.\n",
    "\n",
    "    args:\n",
    "        questions (List[string]): list of questions to be answered\n",
    "        contexts (List[string]): list of context paragraphs, each for answering a question in the input questions\n",
    "        device (torch.device) : the device (CPU or Cuda) that the model and data should be moved to\n",
    "        model (BertForQuestionAnswering): BERT model to be used for question answering \n",
    "            or None - if None, `bertserini-bert-base-squad` will be loaded\n",
    "        tokenizer (BertTokenizerFast object): tokenizer to be used for encoding questions and contexts\n",
    "            or None - if None, `bertserini-bert-base-squad` will be loaded\n",
    "    return:\n",
    "        outputs (List[string]): list of generated answers\n",
    "    '''\n",
    "    \n",
    "    if model is None:\n",
    "        model = BertForQuestionAnswering.from_pretrained('rsvp-ai/bertserini-bert-base-squad')\n",
    "    if tokenizer is None:\n",
    "        tokenizer = BertTokenizerFast.from_pretrained('rsvp-ai/bertserini-bert-base-squad')\n",
    "    \n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    outputs = []\n",
    "\n",
    "    for question, context in tqdm(zip(questions, contexts), total=len(questions)):\n",
    "\n",
    "        encoded_seq = tokenizer(question, context, padding=\"longest\", truncation=True, max_length=512)\n",
    "\n",
    "        tokens = tokenizer.convert_ids_to_tokens(encoded_seq[\"input_ids\"])\n",
    "        \n",
    "        input_ids = torch.LongTensor([encoded_seq[\"input_ids\"]]).to(device)\n",
    "        token_type_ids = torch.LongTensor([encoded_seq[\"token_type_ids\"]]).to(device)\n",
    "        attention_mask = torch.FloatTensor([encoded_seq[\"attention_mask\"]]).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(input_ids=input_ids, \n",
    "                           attention_mask=attention_mask, \n",
    "                           token_type_ids=token_type_ids)\n",
    "        logits_start, logits_end = output['start_logits'], output['end_logits']\n",
    "        token_start = torch.argmax(logits_start)\n",
    "        token_end = torch.argmax(logits_end)\n",
    "        \n",
    "        outputs.append(tokenizer.convert_tokens_to_string(tokens[token_start:token_end]))\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try predicting one row of data first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question:\n",
      "How many awards was Beyonce nominated for at the 52nd Grammy Awards?\n",
      "context:\n",
      "At the 52nd Annual Grammy Awards, Beyoncé received ten nominations, including Album of the Year for I Am... Sasha Fierce, Record of the Year for \"Halo\", and Song of the Year for \"Single Ladies (Put a Ring on It)\", among others. She tied with Lauryn Hill for most Grammy nominations in a single year by a female artist. In 2010, Beyoncé was featured on Lady Gaga's single \"Telephone\" and its music video. The song topped the US Pop Songs chart, becoming the sixth number-one for both Beyoncé and Gaga, tying them with Mariah Carey for most number-ones since the Nielsen Top 40 airplay chart launched in 1992. \"Telephone\" received a Grammy Award nomination for Best Pop Collaboration with Vocals.\n",
      "answer:\n",
      "ten\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 22.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bert prediction: ['ten']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"question:\\n{}\\ncontext:\\n{}\\nanswer:\\n{}\\n\".format(\n",
    "    df_squad.loc[200, \"question\"],\n",
    "    df_squad.loc[200, \"context_paragraph\"],\n",
    "    df_squad.loc[200, \"answer\"]\n",
    "))\n",
    "\n",
    "print(\"Bert prediction:\", get_bert_prediction(\n",
    "    df_squad.loc[[200], \"question\"],\n",
    "    df_squad.loc[[200], \"context_paragraph\"],\n",
    "    device,\n",
    "    model = tuned_model\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see the pretrained model did not work well at all, while fine-tuning the model for one epoch already yields a large improvement in accuracy (about 58%) on the test set. Naturally, the model still has plenty of room for improvement when trained for more epochs, but doing so requires a lot more time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Deployment\n",
    "As seen from this project, building a QA system involves a very complex technology stack. To make your model easily accessible to others, you can deploy it to a public endpoint on Azure, using the same workflow from Project 6. We have built several models so far and they can all be deployed together under the same endpoint. However, for the rest of this project, let's focus on Bert deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "from azureml.core.workspace import Workspace\n",
    "from azureml.core.model import Model\n",
    "from azureml.core.webservice import AciWebservice\n",
    "from azureml.core.model import InferenceConfig\n",
    "from azureml.core.webservice import Webservice\n",
    "from azureml.core.environment import Environment\n",
    "from azureml.core.webservice import LocalWebservice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will provide a brief reminder of the involved steps. You can consult the code from your Project 6 or the [model deployment primer](https://nbviewer.jupyter.org/url/clouddatascience.blob.core.windows.net/primers/p5-machine-learning-azure-primer/azure_model_deployment_primer.ipynb) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1: Initialize workspace with `Workspace.from_config()`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "workspace = Workspace.from_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2: Register the model with `Model.register()`**\n",
    "\n",
    "The `model_path` parameter should be set to the name of your Bert model directory, `\"./bert_fine_tuned_squad\"`. The `model_name` parameter should be set to `\"bert_fine_tuned\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registering model bert_fine_tuned\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "bert_model = Model.register(workspace = workspace, model_path = \"./bert_fine_tuned_squad\", model_name = \"bert_fine_tuned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3: Create scoring script and API endpoint**\n",
    "\n",
    "Our grader will send a POST request to your endpoint, where the JSON content is structured as follows:\n",
    "\n",
    "```py\n",
    "{\n",
    "    \"questions\" : [\n",
    "        \"How many parameters does BERT-large have?\",\n",
    "        \"When did Beyonce start becoming popular?\"\n",
    "    ],\n",
    "\n",
    "    \"context_paragraphs\" : [\n",
    "        \"BERT-large is really big... it has 24-layers and an embedding size of 1,024, for a total of 340M parameters! Altogether it is 1.34GB, so expect it to take a couple minutes to download to your Colab instance.\",\n",
    "        'Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny\\'s Child. Managed by her father, Mathew Knowles, the group became one of the world\\'s best-selling girl groups of all time. Their hiatus saw the release of Beyoncé\\'s debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".'\n",
    "    ]\n",
    "}\n",
    "```\n",
    "Here `questions` is a list of `N` questions and `context_paragraphs` is a list of `N` context paragraphs. For each pair of question and context paragraph, you should tokenize and input them to the fine-tuned Bert model, and return a JSON with the following format:\n",
    "```py\n",
    "{\n",
    "    'predicted_ans' : ['340', 'late 1990s']\n",
    "}\n",
    "```\n",
    "where the key `predict_ans` maps to a list of `N` strings, with each string being the predicted answer for one input pair of question and context paragraph.\n",
    "\n",
    "Implement the `init` and `run` function in the `score.py` file to perform the above processing. In particular,\n",
    "* `init` will load the fine-tuned Bert model from file and store it in a global variable.\n",
    "* `run` will convert the input `input_data` to JSON, extract the questions and contexts, then perform inference and return the specified JSOn response. You can reuse the code from `get_bert_predictions` that we provided earlier.\n",
    "\n",
    "**Notes**:\n",
    "* To load the fine tuned Bert model, you can call\n",
    "```py\n",
    "BertForQuestionAnswering.from_pretrained(Model.get_model_path(\"bert_fine_tuned\"))\n",
    "```\n",
    "Also remember that to modify a global variable, you need to use the `global` keyword.\n",
    "* When doing inference you can use the pre-trained tokenizer `\"rsvp-ai/bertserini-bert-base-squad\"`. This tokenizer can also be initialized in `init` so that you don't need to take time loading it during inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting score.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile score.py\n",
    "import os, json, pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizerFast\n",
    "from transformers import BertForQuestionAnswering\n",
    "from azureml.core.model import Model\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def init():\n",
    "    \"\"\"\n",
    "    Load the fine-tuned Bert model from file and store it in a global variable.\n",
    "    \"\"\"\n",
    "    global model\n",
    "    model = BertForQuestionAnswering.from_pretrained(Model.get_model_path(\"bert_fine_tuned\"))\n",
    "    pass\n",
    "\n",
    "def run(input_data):\n",
    "    \"\"\"\n",
    "    Convert the input data from string to JSON, extract the questions and contexts,\n",
    "    then perform inference with Bert and return the specified JSOn response \n",
    "    \"\"\"\n",
    "    input_json = json.loads(input_data)\n",
    "    questions, contexts = input_json['questions'], input_json['context_paragraphs']\n",
    "    tokenizer = BertTokenizerFast.from_pretrained('rsvp-ai/bertserini-bert-base-squad')\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    outputs = []\n",
    "\n",
    "    for question, context in tqdm(zip(questions, contexts), total=len(questions)):\n",
    "\n",
    "        encoded_seq = tokenizer(question, context, padding=\"longest\", truncation=True, max_length=512)\n",
    "\n",
    "        tokens = tokenizer.convert_ids_to_tokens(encoded_seq[\"input_ids\"])\n",
    "        \n",
    "        input_ids = torch.LongTensor([encoded_seq[\"input_ids\"]]).to(device)\n",
    "        token_type_ids = torch.LongTensor([encoded_seq[\"token_type_ids\"]]).to(device)\n",
    "        attention_mask = torch.FloatTensor([encoded_seq[\"attention_mask\"]]).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(input_ids=input_ids, \n",
    "                           attention_mask=attention_mask, \n",
    "                           token_type_ids=token_type_ids)\n",
    "        logits_start, logits_end = output['start_logits'], output['end_logits']\n",
    "        token_start = torch.argmax(logits_start)\n",
    "        token_end = torch.argmax(logits_end)\n",
    "        \n",
    "        outputs.append(tokenizer.convert_tokens_to_string(tokens[token_start:token_end]))\n",
    "    predict_json = {'predicted_ans': outputs}\n",
    "    return predict_json\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now check that the content of `score.py` is as you expect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "import os, json, pickle\r\n",
      "import numpy as np\r\n",
      "import torch\r\n",
      "from tqdm import tqdm\r\n",
      "from transformers import BertTokenizerFast\r\n",
      "from transformers import BertForQuestionAnswering\r\n",
      "from azureml.core.model import Model\r\n",
      "\r\n",
      "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\r\n",
      "\r\n",
      "def init():\r\n",
      "    \"\"\"\r\n",
      "    Load the fine-tuned Bert model from file and store it in a global variable.\r\n",
      "    \"\"\"\r\n",
      "    global model\r\n",
      "    model = BertForQuestionAnswering.from_pretrained(Model.get_model_path(\"bert_fine_tuned\"))\r\n",
      "    pass\r\n",
      "\r\n",
      "def run(input_data):\r\n",
      "    \"\"\"\r\n",
      "    Convert the input data from string to JSON, extract the questions and contexts,\r\n",
      "    then perform inference with Bert and return the specified JSOn response \r\n",
      "    \"\"\"\r\n",
      "    input_json = json.loads(input_data)\r\n",
      "    questions, contexts = input_json['questions'], input_json['context_paragraphs']\r\n",
      "    tokenizer = BertTokenizerFast.from_pretrained('rsvp-ai/bertserini-bert-base-squad')\r\n",
      "    model.to(device)\r\n",
      "    model.eval()\r\n",
      "    \r\n",
      "    outputs = []\r\n",
      "\r\n",
      "    for question, context in tqdm(zip(questions, contexts), total=len(questions)):\r\n",
      "\r\n",
      "        encoded_seq = tokenizer(question, context, padding=\"longest\", truncation=True, max_length=512)\r\n",
      "\r\n",
      "        tokens = tokenizer.convert_ids_to_tokens(encoded_seq[\"input_ids\"])\r\n",
      "        \r\n",
      "        input_ids = torch.LongTensor([encoded_seq[\"input_ids\"]]).to(device)\r\n",
      "        token_type_ids = torch.LongTensor([encoded_seq[\"token_type_ids\"]]).to(device)\r\n",
      "        attention_mask = torch.FloatTensor([encoded_seq[\"attention_mask\"]]).to(device)\r\n",
      "\r\n",
      "        with torch.no_grad():\r\n",
      "            output = model(input_ids=input_ids, \r\n",
      "                           attention_mask=attention_mask, \r\n",
      "                           token_type_ids=token_type_ids)\r\n",
      "        logits_start, logits_end = output['start_logits'], output['end_logits']\r\n",
      "        token_start = torch.argmax(logits_start)\r\n",
      "        token_end = torch.argmax(logits_end)\r\n",
      "        \r\n",
      "        outputs.append(tokenizer.convert_tokens_to_string(tokens[token_start:token_end]))\r\n",
      "    predict_json = {'predicted_ans': outputs}\r\n",
      "    return predict_json\r\n",
      "    pass\r\n"
     ]
    }
   ],
   "source": [
    "!cat score.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4: Create environment file**\n",
    "\n",
    "Now you will need to create an environment file (`myenv.yml`) that specifies all of the scoring script's package dependencies. This file is used to ensure that all of those dependencies are installed in the Docker image by Azure ML. The `pip_packages` parameter value should be the following list:\n",
    "```py\n",
    "['azureml-defaults', 'torch==1.9.0', 'transformers==4.7.0', 'numpy']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "from azureml.core.conda_dependencies import CondaDependencies \n",
    "env_file = CondaDependencies.create(pip_packages=['azureml-defaults', 'torch==1.9.0', 'transformers==4.7.0', 'numpy'])\n",
    "with open('myenv.yml', 'w') as file:\n",
    "    file.write(env_file.serialize_to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 5: Deploy to local service**\n",
    "\n",
    "Follow the steps in the [model deloyment primer](https://nbviewer.jupyter.org/url/clouddatascience.blob.core.windows.net/primers/machine-learning-azure-primer/azure_model_deployment_primer.ipynb) to create an `Environment`, an `InferenceConfig`, a `LocalWebservice`, a `Model.deploy` object, and call `wait_for_deployment` on it. This code may take about 10 minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading model bert_fine_tuned:2 to /tmp/azureml_rf88aogp/bert_fine_tuned/2\n",
      "Generating Docker build context.\n",
      "Package creation Succeeded\n",
      "Logging into Docker registry a1ec990464d94e6286f26e9d5274f1fe.azurecr.io\n",
      "Logging into Docker registry a1ec990464d94e6286f26e9d5274f1fe.azurecr.io\n",
      "Building Docker image from Dockerfile...\n",
      "Step 1/5 : FROM a1ec990464d94e6286f26e9d5274f1fe.azurecr.io/azureml/azureml_0d014b9c79122f4b244bba591b5ba5c9\n",
      " ---> 9acfcf1b2a2a\n",
      "Step 2/5 : COPY azureml-app /var/azureml-app\n",
      " ---> 9300f72d75a7\n",
      "Step 3/5 : RUN mkdir -p '/var/azureml-app' && echo eyJhY2NvdW50Q29udGV4dCI6eyJzdWJzY3JpcHRpb25JZCI6IjAwYjcxMGRjLWJiN2ItNGQ3MC1hYWE1LTk3YjY2MjcwMDQyNCIsInJlc291cmNlR3JvdXBOYW1lIjoicHJvamVjdDciLCJhY2NvdW50TmFtZSI6InByb2plY3Q3Iiwid29ya3NwYWNlSWQiOiJhMWVjOTkwNC02NGQ5LTRlNjItODZmMi02ZTlkNTI3NGYxZmUifSwibW9kZWxzIjp7fSwibW9kZWxzSW5mbyI6e319 | base64 --decode > /var/azureml-app/model_config_map.json\n",
      " ---> Running in 10a68fb4d3af\n",
      " ---> e3ca6a690e73\n",
      "Step 4/5 : RUN mv '/var/azureml-app/tmpbmh4jsgz.py' /var/azureml-app/main.py\n",
      " ---> Running in 512c8e78c125\n",
      " ---> 4e26f8ba7794\n",
      "Step 5/5 : CMD [\"runsvdir\",\"/var/runit\"]\n",
      " ---> Running in 3906fbc7f537\n",
      " ---> 484c19006584\n",
      "Successfully built 484c19006584\n",
      "Successfully tagged local-service-bert-3:latest\n",
      "Starting Docker container...\n",
      "Docker container running.\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "\n",
    "myenv = Environment.from_conda_specification(name = 'myenv', file_path = 'myenv.yml')\n",
    "\n",
    "inference_config = InferenceConfig(source_directory = '.', entry_script = 'score.py', environment = myenv)\n",
    "\n",
    "local_deployment_target = LocalWebservice.deploy_configuration(port = 8891)\n",
    "\n",
    "\n",
    "local_service_bert = Model.deploy(\n",
    "    workspace, 'local-service-bert-3',\n",
    "    [bert_model], inference_config, \n",
    "    local_deployment_target\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 6: Test the local service**\n",
    "\n",
    "You should change the `local_service` variable below to the variable name that you used earlier to store the return value of `Model.deploy`. Check that the response JSON matches the format specified above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking container health...\n",
      "Local webservice is running at http://localhost:8891\n",
      "{'predicted_ans': ['340m', 'late 1990s']}\n"
     ]
    }
   ],
   "source": [
    "input_json = json.dumps({\n",
    "    \"questions\" : [\n",
    "        \"How many parameters does BERT-large have?\",\n",
    "        \"When did Beyonce start becoming popular?\"\n",
    "    ],\n",
    "\n",
    "    \"context_paragraphs\" : [\n",
    "        \"BERT-large is really big... it has 24-layers and an embedding size of 1,024, for a total of 340M parameters! Altogether it is 1.34GB, so expect it to take a couple minutes to download to your Colab instance.\",\n",
    "        'Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny\\'s Child. Managed by her father, Mathew Knowles, the group became one of the world\\'s best-selling girl groups of all time. Their hiatus saw the release of Beyoncé\\'s debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".'\n",
    "    ]\n",
    "})\n",
    "\n",
    "input_json = bytes(input_json, encoding = \"utf8\")\n",
    "output = local_service_bert.run(input_json)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that your model may output `340` or `340m` as the answer for the first question -- both are acceptable, due to the random weights in the Bert model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 7: Deploy to ACI container**\n",
    "\n",
    "Now that your local service has worked properly, the last step is to deploy it to a pulic endpoint. Follow the steps in the primer to create an `AciWebService` and a new `Model.deploy` object, then call `wait_for_deployment` on it. This code should take at most 10 minutes to run. If it takes longer, you should consult the \"Monitoring public deployment\" section in the primer to diagnose the issues.\n",
    "\n",
    "**Notes**:\n",
    "* When creating the `AciWebService`, you can specify `cpu_cores = 3.8` and `memory_gb = 15` to maximize processing power in the deployment environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Tips: You can try get_logs(): https://aka.ms/debugimage#dockerlog or local deployment: https://aka.ms/debugimage#debug-locally to debug if deployment takes longer than 10 minutes.\n",
      "Running\n",
      "2021-12-05 02:58:27+00:00 Creating Container Registry if not exists.\n",
      "2021-12-05 02:58:27+00:00 Registering the environment.\n",
      "2021-12-05 02:58:28+00:00 Use the existing image.\n",
      "2021-12-05 02:58:29+00:00 Generating deployment configuration.\n",
      "2021-12-05 02:58:30+00:00 Submitting deployment to compute.\n",
      "2021-12-05 02:58:33+00:00 Checking the status of deployment bert-aci-service-1..\n",
      "2021-12-05 03:01:33+00:00 Checking the status of inference endpoint bert-aci-service-1.\n",
      "Succeeded\n",
      "ACI service creation operation finished, operation \"Succeeded\"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "aci_deployment_config = AciWebservice.deploy_configuration(\n",
    "    cpu_cores=3.8, memory_gb=15, description = 'Public endpoint for bert model')\n",
    "\n",
    "bert_aci_service = Model.deploy(\n",
    "    workspace, \"bert-aci-service-1\",\n",
    "    [bert_model], inference_config, \n",
    "    aci_deployment_config\n",
    ")\n",
    "\n",
    "bert_aci_service.wait_for_deployment(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 8: Test the public service**\n",
    "\n",
    "Let's use the same input json from earlier and check that the public endpoint is working as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://628b2a58-8a27-49f3-b900-1ea6daba35bc.eastus.azurecontainer.io/score\n",
      "200\n",
      "{'predicted_ans': ['340m', 'late 1990s']}\n"
     ]
    }
   ],
   "source": [
    "deployed_uri = bert_aci_service.scoring_uri\n",
    "print(deployed_uri)\n",
    "\n",
    "input_json = json.dumps({\n",
    "    \"questions\" : [\n",
    "        \"How many parameters does BERT-large have?\",\n",
    "        \"When did Beyonce start becoming popular?\"\n",
    "    ],\n",
    "\n",
    "    \"context_paragraphs\" : [\n",
    "        \"BERT-large is really big... it has 24-layers and an embedding size of 1,024, for a total of 340M parameters! Altogether it is 1.34GB, so expect it to take a couple minutes to download to your Colab instance.\",\n",
    "        'Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny\\'s Child. Managed by her father, Mathew Knowles, the group became one of the world\\'s best-selling girl groups of all time. Their hiatus saw the release of Beyoncé\\'s debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".'\n",
    "    ]\n",
    "})\n",
    "\n",
    "response = requests.post(deployed_uri, input_json, headers = {'Content-Type' : 'application/json'})\n",
    "print(response.status_code)\n",
    "print(json.loads(response.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 9: Deploying Bert model\n",
    "Here is a locak test to check our deployed model can handle a request that contains 100 data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed!\n"
     ]
    }
   ],
   "source": [
    "def test_deploy_bert():\n",
    "    df_test_deploy = df_squad.sample(100, random_state = 100)\n",
    "    input_json = json.dumps({\n",
    "        \"questions\" : df_test_deploy[\"question\"].tolist(),\n",
    "        \"context_paragraphs\" : df_test_deploy[\"context_paragraph\"].tolist()\n",
    "    })\n",
    "    response = requests.post(deployed_uri, input_json, headers = {'Content-Type' : 'application/json'})\n",
    "    check_equal(response.status_code, 200)\n",
    "    \n",
    "    predicted_ans = np.array(json.loads(response.content)[\"predicted_ans\"])\n",
    "    ans = df_test_deploy[\"answer\"].str.lower().values\n",
    "    accuracy = (predicted_ans == ans).mean()\n",
    "    assert accuracy >= 0.3, accuracy\n",
    "    print(\"All tests passed!\")\n",
    "    \n",
    "test_deploy_bert()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernel_info": {
   "name": "python38-azureml"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
